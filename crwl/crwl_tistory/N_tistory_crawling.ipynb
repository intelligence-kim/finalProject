{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "service = Service(executable_path=ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_tistory_links(keyword_input):\n",
    "''' input에 검색키워드를 넣으면 tistory에서 검색하여\n",
    "    url 링크들을 따오는 함수\n",
    "'''\n",
    "    browser = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    browser.maximize_window()\n",
    "    url = f\"https://www.tistory.com/m/search?query={keyword_input}&tab=post\"\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(2)\n",
    "    \n",
    "    for i in range(20): #스크롤 20번 내림 약 560개의 url 가져옴\n",
    "        browser.find_element(By.CSS_SELECTOR, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(1)\n",
    "        browser.execute_script(\"return window.scrollY\")\n",
    "    \n",
    "    links = browser.find_elements(By.CSS_SELECTOR, \"ul.sch_list_post > li > a\")\n",
    "    links = [_.get_attribute('href') for _ in links]\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_tistory_blogs(links):\n",
    "'''\n",
    "crawl_tistory_links로 뽑아온 url을 담은 links를 이용\n",
    "BS로 요청하여 title, text를 뽑아옴\n",
    "주석처리는 selenium 사용했던 것들\n",
    "'''\n",
    "    crwl_blog_text = []\n",
    "    titles = []\n",
    "#    browser = webdriver.Chrome(service=service, options=chrome_options)\n",
    "#    browser.set_page_load_timeout(7)\n",
    "    # browser.Manage().Timeouts().PageLoad = TimeSpan.FromMinutes(3)\n",
    "    tk0 = tqdm(range(len(links)), smoothing=0, mininterval=1.0)\n",
    "    for i in tk0:\n",
    "        try:\n",
    "            # browser.set_page_load_timeout(10)\n",
    "            blog_text = ''\n",
    "            # browser.implicitly_wait(2.5)\n",
    "            #browser.get(links[i])\n",
    "            \n",
    "            #WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            response = requests.get(links[i])\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            #soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            title = soup.find(\"h3\", class_=\"tit_blogview\")\n",
    "            div = soup.find(\"div\", class_=\"blogview_content useless_p_margin editor_ke\") # 본문위 class 종류가 2개라 2개 다 불러오고 합침\n",
    "            div2 = soup.find(\"div\", class_=\"blogview_content editor_ke\")#\n",
    "            combined_str = str(div) + str(div2)\n",
    "            combined_soup = BeautifulSoup(combined_str, 'html.parser')\n",
    "            text = combined_soup.find_all('p')\n",
    "            text = [_ for _ in text if not _.find('a')]\n",
    "            for t in text:\n",
    "                blog_text += t.text\n",
    "            if blog_text is not None: #본문 text가 존재하면 append 아니면 'None'을 추가\n",
    "                crwl_blog_text.append(blog_text)\n",
    "            else:\n",
    "                crwl_blog_text.append('None')\n",
    "            if title is not None: #title가 존재하면 append 아니면 'None'을 추가\n",
    "                titles.append(title.text)\n",
    "            else:\n",
    "                titles.append('None')\n",
    "        except WebDriverException:\n",
    "            print(links[i],'WebDriverException에러 발생')\n",
    "            pass\n",
    "        except TimeoutException:\n",
    "            print('Timeout에러 발생')\n",
    "            pass\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(links[i],\"connectionError 발생\")\n",
    "            pass\n",
    "    crwl_blog_text = [re.sub('\\n+', '',text) for text in crwl_blog_text] #전처리\n",
    "    \n",
    "    return crwl_blog_text, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd08875e1ca042f0862c7d121f6545b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://11231004.com/m/entry/enfj-%EC%9C%A0%ED%98%95-%ED%8A%B9%EC%A7%95-%EC%9E%A5%EB%8B%A8%EC%A0%90-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0 connectionError 발생\n",
      "https://mbti.icrdt.com/m/49 connectionError 발생\n",
      "https://mbti.icrdt.com/m/26 connectionError 발생\n",
      "https://11231004.com/m/entry/ISFJ-%ED%8A%B9%EC%A7%95-%EA%B6%81%ED%95%A9-%EC%9E%90%EC%84%B8%ED%95%98%EA%B2%8C-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0 connectionError 발생\n"
     ]
    }
   ],
   "source": [
    "search_keywords = ['ENFJ 특징'] #검색키워드\n",
    "for keyword in search_keywords:\n",
    "    links = crawl_tistory_links(keyword)\n",
    "    crwl_blog_text, titles = crawl_tistory_blogs(links)\n",
    "    df = pd.DataFrame({'title': titles, 'content': crwl_blog_text})\n",
    "    df.to_csv(f'{keyword[:4]}_tistory_crawling.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     오늘은 MBTI 유형중, ISFP 유형의 특징을 총정리하는 시간을 갖도록 하겠습니다...\n",
       "1      엠비티아이 잇프피 유형의 특징에 대해 알아보도록 하겠습니다. ISFP 유형은 조용...\n",
       "2       ISFP는 강한 예술적 측면을 가지고 있습니다. 그들은 시각 예술, 음악 또는 ...\n",
       "3     MBTI 유형 중에서도 조용하고 차분한 성격인 ISFP유형입니다. 이 유형은 내향형...\n",
       "4     MBTI 성격 유형 중 ISFP는 \"호기심 많은 예술가\"로 알려져 있습니다. 이들은...\n",
       "5      목차 ISFP유형은 Myers-Briggs 유형 지표(MBTI)에서 사용되는 성격...\n",
       "6     ISFP 유형 특징에 대해서 알아보도록 할게요!ISFP-ISFP 특징--ISFP 장...\n",
       "7     대체적으로 보이는 특징은 아래와 같아요.●감성적: ISFP는 자신의 감정을 중요하게...\n",
       "8     호기심 많은 예술가, 성인군자형인 ISFP입니다.말없이 다정하고 온화하며 사람들에게...\n",
       "9     ISFP 특징 팩폭 연예인 등 ISFP에 대해 알아보도록 하겠습니다.MBTI는 My...\n",
       "10                                                  NaN\n",
       "11    isfp유형의문의 인싸라고 불리는 ISFP유형은 I(내향형) + S(감각형) + F...\n",
       "12    ISFP 유형은 조용하고 친절하며인내심이 강한 성격입니다.말보다 행동으로 나타내는 ...\n",
       "13    매우 섬세하며 따뜻한 배려가 정말 매력적인 여자들입니다. 친절하고 겸손하며 늘 상대...\n",
       "14    ISFP는 민감하고, 창의적이며, 그들의 감정에 깊이 적응합니다. 관계에서 ISFP...\n",
       "15    이번 게시글에서는 ISFP 특징에 대해 알아보도록 하겠다. ISFP는 내향적 감각형...\n",
       "16    [ISFP 유형] 특징 총정리 ISFP형은 점잖고 감수성이 많으며 인정이 많습니다....\n",
       "17    이번 게시글에서는 ISFP 남자 특징에 대해 알아보도록 하겠다. ISFP 남자는 M...\n",
       "18    안녕하세요! 이번에는 MBTI 유형 중 ISFP에 대해 알아볼게요.ISFP는 예술가...\n",
       "19    모험가라는 별명을 가진 ISFP는 내향 + 감각 + 감정 + 인식으로 구성된 MBT...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ISFP_tistory_crawling.csv')\n",
    "df.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
