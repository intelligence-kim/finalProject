{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from mxnet) (1.22.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from mxnet) (2.26.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet) (3.2)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.20.1\n",
      "    Uninstalling graphviz-0.20.1:\n",
      "      Successfully uninstalled graphviz-0.20.1\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
      "Collecting gluonnlp==0.9.1\n",
      "  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from gluonnlp==0.9.1) (1.22.4)\n",
      "Requirement already satisfied: cython in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from gluonnlp==0.9.1) (0.29.24)\n",
      "Requirement already satisfied: packaging in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from gluonnlp==0.9.1) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from packaging->gluonnlp==0.9.1) (3.0.4)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp39-cp39-linux_x86_64.whl size=362442 sha256=bb952b5db676d18435ca9c2b1274787ee5b8ed8363925beca0fbdefc4f890e3f\n",
      "  Stored in directory: /home/studio/.cache/pip/wheels/f0/1a/57/1fac92705aa8b074509a0f514fc0cb749ca6332feb3901692d\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.9.1\n",
      "Requirement already satisfied: sentencepiece in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (0.1.96)\n",
      "Requirement already satisfied: transformers in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (4.27.4)\n",
      "Requirement already satisfied: filelock in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: torch in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n",
      "Requirement already satisfied: wheel in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: cmake in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio/anaconda3/envs/project/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp==0.9.1\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-pmirqvou/kobert-tokenizer_c3354c6bfdb3441f82b50dc6795aef74\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-pmirqvou/kobert-tokenizer_c3354c6bfdb3441f82b50dc6795aef74\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: kobert_tokenizer\n",
      "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4648 sha256=eb77a1b19e874462685e10c0dbcdf2eabaabf9eec0106c561d766409972679b2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g2bfr_n4/wheels/64/c4/82/d70d864682468fad0bf8269b0dfd88daf48f209be4c8ade164\n",
      "Successfully built kobert_tokenizer\n",
      "Installing collected packages: kobert_tokenizer\n",
      "Successfully installed kobert_tokenizer-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import torch\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', sp_model_kwargs={'nbest_size': -1, 'alpha': 0.6, 'enable_sampling': True})\n",
    "tokenizer.encode(\"한국어 모델을 공유합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "text = \"한국어 모델을 공유합니다.\"\n",
    "inputs = tokenizer.batch_encode_plus([text])\n",
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-13 00:42:52--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
      "www.dropbox.com (www.dropbox.com) 해석 중... 162.125.84.18, 2620:100:6034:18::a27d:5412\n",
      "다음으로 연결 중: www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: /s/dl/374ftkec978br3d/ratings_train.txt [따라감]\n",
      "--2023-07-13 00:42:52--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n",
      "www.dropbox.com에 기존 연결 재활용:443.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: https://uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com/cd/0/get/B_tdnOk2XjqOwtZSyap6EePz8E5Qx339EuWf0mHyOn97flp26cDWZ3TG_Ucz6sTnji66nldSC2BmJgL_9BFPGZ8zDnb_h0uZn7joulBwDDH4Bs4qpWMahqwgERIkanKgS0yS_H-8WHvZ3LFd-Vs6y95o7gs5XRNVq-WVLomWmE2bzg/file?dl=1# [따라감]\n",
      "--2023-07-13 00:42:52--  https://uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com/cd/0/get/B_tdnOk2XjqOwtZSyap6EePz8E5Qx339EuWf0mHyOn97flp26cDWZ3TG_Ucz6sTnji66nldSC2BmJgL_9BFPGZ8zDnb_h0uZn7joulBwDDH4Bs4qpWMahqwgERIkanKgS0yS_H-8WHvZ3LFd-Vs6y95o7gs5XRNVq-WVLomWmE2bzg/file?dl=1\n",
      "uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com (uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com) 해석 중... 162.125.84.15, 2620:100:6034:15::a27d:540f\n",
      "다음으로 연결 중: uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com (uc0315e0bfe5c1579e1beea6197a.dl.dropboxusercontent.com)|162.125.84.15|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 14628807 (14M) [application/binary]\n",
      "저장 위치: ‘ratings_train.txt?dl=1’\n",
      "\n",
      "ratings_train.txt?d 100%[===================>]  13.95M  22.2MB/s    / 0.6s     \n",
      "\n",
      "2023-07-13 00:42:53 (22.2 MB/s) - ‘ratings_train.txt?dl=1’ 저장함 [14628807/14628807]\n",
      "\n",
      "--2023-07-13 00:42:54--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
      "www.dropbox.com (www.dropbox.com) 해석 중... 162.125.84.18, 2620:100:6034:18::a27d:5412\n",
      "다음으로 연결 중: www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: /s/dl/977gbwh542gdy94/ratings_test.txt [따라감]\n",
      "--2023-07-13 00:42:54--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n",
      "www.dropbox.com에 기존 연결 재활용:443.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: https://uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com/cd/0/get/B_vzEeJv3Wbi84aqI1apVwNF6kXMLU-IhvcRLFKT1LzdXVZyAQt9xzSAd-t2y4cv3tozR-kWy1whxETjs2gTtpmkbXLJgJPN7q7gZ-smbSnVpqm7oIzScqbsLwDzr9wmyInHArcdva8-o9zPyxq0Jivnky4mneA8YcE0kllbeRSiLQ/file?dl=1# [따라감]\n",
      "--2023-07-13 00:42:54--  https://uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com/cd/0/get/B_vzEeJv3Wbi84aqI1apVwNF6kXMLU-IhvcRLFKT1LzdXVZyAQt9xzSAd-t2y4cv3tozR-kWy1whxETjs2gTtpmkbXLJgJPN7q7gZ-smbSnVpqm7oIzScqbsLwDzr9wmyInHArcdva8-o9zPyxq0Jivnky4mneA8YcE0kllbeRSiLQ/file?dl=1\n",
      "uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com (uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com) 해석 중... 162.125.84.15, 2620:100:6034:15::a27d:540f\n",
      "다음으로 연결 중: uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com (uc44c0f72b9b7512a310af545f13.dl.dropboxusercontent.com)|162.125.84.15|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 4893335 (4.7M) [application/binary]\n",
      "저장 위치: ‘ratings_test.txt?dl=1’\n",
      "\n",
      "ratings_test.txt?dl 100%[===================>]   4.67M  14.7MB/s    / 0.3s     \n",
      "\n",
      "2023-07-13 00:42:55 (14.7 MB/s) - ‘ratings_test.txt?dl=1’ 저장함 [4893335/4893335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.tokenize\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio/anaconda3/envs/project/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dee488c1c0b4dd19cfd8803c7bc46df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.6927973628044128 train acc 0.640625\n",
      "epoch 1 batch id 201 loss 0.5581398606300354 train acc 0.5772699004975125\n",
      "epoch 1 batch id 401 loss 0.4007514715194702 train acc 0.6817721321695761\n",
      "epoch 1 batch id 601 loss 0.4294225871562958 train acc 0.733205074875208\n",
      "epoch 1 batch id 801 loss 0.4648691713809967 train acc 0.7625624219725343\n",
      "epoch 1 batch id 1001 loss 0.28572356700897217 train acc 0.7798607642357642\n",
      "epoch 1 batch id 1201 loss 0.30930984020233154 train acc 0.7932452123230641\n",
      "epoch 1 batch id 1401 loss 0.3249175548553467 train acc 0.801960653104925\n",
      "epoch 1 batch id 1601 loss 0.28079214692115784 train acc 0.8102943472829481\n",
      "epoch 1 batch id 1801 loss 0.2638770341873169 train acc 0.816777137701277\n",
      "epoch 1 batch id 2001 loss 0.2757550776004791 train acc 0.8224481509245377\n",
      "epoch 1 batch id 2201 loss 0.29596811532974243 train acc 0.8274363925488414\n",
      "epoch 1 train acc 0.8309913609215017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5d3ce8fb24eaa9e5fc27bd8dcd300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8860294117647058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023aadf588a1425494e858c110479770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.524494469165802 train acc 0.78125\n",
      "epoch 2 batch id 201 loss 0.23066921532154083 train acc 0.8804415422885572\n",
      "epoch 2 batch id 401 loss 0.324258953332901 train acc 0.8826761221945137\n",
      "epoch 2 batch id 601 loss 0.3711325228214264 train acc 0.8866212562396006\n",
      "epoch 2 batch id 801 loss 0.31626465916633606 train acc 0.8894545880149812\n",
      "epoch 2 batch id 1001 loss 0.25773024559020996 train acc 0.8913898601398601\n",
      "epoch 2 batch id 1201 loss 0.2478388100862503 train acc 0.8942547876769359\n",
      "epoch 2 batch id 1401 loss 0.2674030661582947 train acc 0.8960340827980015\n",
      "epoch 2 batch id 1601 loss 0.3036961257457733 train acc 0.8981788725796377\n",
      "epoch 2 batch id 1801 loss 0.1328956037759781 train acc 0.900159633536924\n",
      "epoch 2 batch id 2001 loss 0.25733837485313416 train acc 0.9020489755122438\n",
      "epoch 2 batch id 2201 loss 0.2706226110458374 train acc 0.9036801453884598\n",
      "epoch 2 train acc 0.9048323734357224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916b1a052a6143f6a0e46f32d641760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8921435421994884\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cd9167712247c5ba19e90662d2244c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.454826295375824 train acc 0.8125\n",
      "epoch 3 batch id 201 loss 0.1580171287059784 train acc 0.9235074626865671\n",
      "epoch 3 batch id 401 loss 0.1653374433517456 train acc 0.9262780548628429\n",
      "epoch 3 batch id 601 loss 0.24713154137134552 train acc 0.92920653078203\n",
      "epoch 3 batch id 801 loss 0.2131827026605606 train acc 0.9313943508114857\n",
      "epoch 3 batch id 1001 loss 0.18293635547161102 train acc 0.9334727772227772\n",
      "epoch 3 batch id 1201 loss 0.1691536009311676 train acc 0.9353273313905079\n",
      "epoch 3 batch id 1401 loss 0.11503888666629791 train acc 0.9367193076374019\n",
      "epoch 3 batch id 1601 loss 0.1996898055076599 train acc 0.9381246096189881\n",
      "epoch 3 batch id 1801 loss 0.09411992877721786 train acc 0.9394520405330372\n",
      "epoch 3 batch id 2001 loss 0.21619005501270294 train acc 0.9409513993003499\n",
      "epoch 3 batch id 2201 loss 0.2367943525314331 train acc 0.9417807246706043\n",
      "epoch 3 train acc 0.9427061113481229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c438c248fe56457b848df543e76760c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8954403772378516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6dbab5025d4a32b1d86f12b5004fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.4254910349845886 train acc 0.859375\n",
      "epoch 4 batch id 201 loss 0.06075352057814598 train acc 0.9571672885572139\n",
      "epoch 4 batch id 401 loss 0.1347247213125229 train acc 0.958658042394015\n",
      "epoch 4 batch id 601 loss 0.17482542991638184 train acc 0.9599885607321131\n",
      "epoch 4 batch id 801 loss 0.176620215177536 train acc 0.9609667602996255\n",
      "epoch 4 batch id 1001 loss 0.09816300123929977 train acc 0.9615072427572428\n",
      "epoch 4 batch id 1201 loss 0.060820695012807846 train acc 0.9627784138218152\n",
      "epoch 4 batch id 1401 loss 0.05569837614893913 train acc 0.9634078336902213\n",
      "epoch 4 batch id 1601 loss 0.13630050420761108 train acc 0.9644460493441599\n",
      "epoch 4 batch id 1801 loss 0.0568542554974556 train acc 0.9655399777901166\n",
      "epoch 4 batch id 2001 loss 0.04539716616272926 train acc 0.966391804097951\n",
      "epoch 4 batch id 2201 loss 0.20046548545360565 train acc 0.9668971490231713\n",
      "epoch 4 train acc 0.96732126350967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0a541f74f4a6ebf5e5f4d47110903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.897258631713555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ee4c043f2f4d838acd7c684cbc1cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.3733232021331787 train acc 0.890625\n",
      "epoch 5 batch id 201 loss 0.02429956942796707 train acc 0.9745802238805971\n",
      "epoch 5 batch id 401 loss 0.10971395671367645 train acc 0.9757247506234414\n",
      "epoch 5 batch id 601 loss 0.12054506689310074 train acc 0.9763415141430949\n",
      "epoch 5 batch id 801 loss 0.19424889981746674 train acc 0.9765917602996255\n",
      "epoch 5 batch id 1001 loss 0.011286986991763115 train acc 0.977662962037962\n",
      "epoch 5 batch id 1201 loss 0.027933500707149506 train acc 0.9782212739383847\n",
      "epoch 5 batch id 1401 loss 0.07862568646669388 train acc 0.9783636688079943\n",
      "epoch 5 batch id 1601 loss 0.05443156510591507 train acc 0.9788803872579638\n",
      "epoch 5 batch id 1801 loss 0.008495762944221497 train acc 0.9792216129927818\n",
      "epoch 5 batch id 2001 loss 0.007236095145344734 train acc 0.9794946276861569\n",
      "epoch 5 batch id 2201 loss 0.09542693942785263 train acc 0.9796328373466606\n",
      "epoch 5 train acc 0.9797354948805461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ba2e087b224fc2b8d71bbc1a03d989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.8973385549872123\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentence):\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    test_model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = test_model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"부정\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"긍정\")\n",
    "            else:\n",
    "                test_eval.append(\"??\")\n",
    "\n",
    "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('mbti_final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "text = literal_eval(mbti['contents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'istj 현실주의자 conclusion 결론 결론 현실주의자만큼 실용적이고 헌신적인 유형은 거의 없습니다. 신뢰성과 노력으로 유명한 현실주의자는 자신과 사랑하는 사람들을 위해 안전하고 안정적인 환경을 형성하고 유지하는 것에 능숙합니다. 현실주의자들의 헌신은 자기계발을 포함한 많은 영역에서 중요하게 활용됩니다. 하지만 현실주의자들의 실용적이고 체계적인 접근방식이 오히려 문제가 되는 상황도 생길 수 있습니다. 연애상대를 찾고 관계를 유지하는 것이나 긴장을 줄이고 융통성있게 배우는 것이나 업무량을 관리하는 것 등 현실주의자들은 스스로의 약점을 극복하고 추가적인 역량 강화를 위해 의식적인 노력을 기울여야 합니다. istj 현실주의자 introduction 소개 포스팅으로 이동'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "test = kkma.sentences(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['동시에 규칙에 따르는 것을 선호하고']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "kkma.sentences(test[-6][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['istj 현실주의자 workplace habits 직장 습관 직장 습관 현실주의자는 직장에서 전형적인 근면하고 성실한 직원입니다.',\n",
       " '어떤 직책에서도 현실주의자들 유형은 명확하게 정의된 규칙과 권위 및 직책에 대한 존중을 추구합니다.',\n",
       " '현실주의자에게 책임은 부담이 아니라 신뢰이며 일에 적합한 사람이라는 것을 증명할 수 있는 기회입니다.',\n",
       " '반면에 새로운 책임을 떠안고 반대로 오래된 책임을 내려놓는 것에 수반되는 변화는 현실주의자들이 어렵게 느끼는 부분 중 하나입니다.',\n",
       " '어떤 직책에 있느냐에 따라 다르게 나타나지만 이들이 극복해야 할 가장 중요한 과제라고 할 수 있습니다.',\n",
       " '현실주의자 유형의 많은 사람들은 개인적이고 전문적인 업무에 집중하는 경우가 많습니다.',\n",
       " '현실주의자 직원 현실주의자들이 책임을 중요시하는 성향은 실제 중요하지 않은 여러 업무를 떠안게 만드는 요인으로 작용하기도 합니다.',\n",
       " '이 들은 모든 분야에서 전문가로 간주되어 메뉴 얼이 정해진 프로젝트를 아주 능숙하게 처리할 수 있는 능력을 가지고 있습니다.',\n",
       " '하지만 과도한 부담감을 느끼거나 일에 더욱 적합한 사람이 있는 경우에도 책임을 포기하는 것을 꺼리게 만드는 경우도 있습니다.',\n",
       " '현실주의자의 업무에 대한 진지함은 비판에 매우 예민 해지게 만들고 업무를 수행하는 과정에서 짜증이 날 수도 있습니다.',\n",
       " '현실주의자는 이처럼 업무에 진지하기 때문에 가장 생산 적인 직원이 될 수 있습니다.',\n",
       " '권위와 위계를 중시하고 명령과 지시를 따르는 것이 뛰어나기 때문입니다.',\n",
       " '이들이 시간을 엄수하고 정시에 출근하고 프로젝트 마감일을 맞추는 것에 문제가 생길 일은 없습니다.',\n",
       " '명확하게 정해진 업무체계와 책임이 함께 있다면 현실주의자들은 매우 충성스럽고 헌신적이며 꼼꼼 하게 일을 추진해 나갑니다.',\n",
       " '현실주의자 동료 동료들 사이에서 현실주의자 만큼 프로젝트를 제시간에 메뉴 얼대로 끝나는 것을 보장할 수 있는 믿음 직 스러 운 사람은 없습니다.',\n",
       " '조용하고 체계적인 현실주의자들은 어떤 일에 문제가 생겼을 때 냉정함을 유지하며 동료들 또한 같은 접근 방식을 공유하기를 기대합니다.',\n",
       " '하지만 감정이 앞서거나 감정적인 지지가 필요한 동료들은 전체적으로 업무의 효율성을 떨어뜨리는 방식으로 현실주의자들을 당황스럽게 하기도 합니다.',\n",
       " '현실주의자들은 직장에서 평화와 안전을 중요시합니다.',\n",
       " '그래서 혼자 일하는 것을 편하게 생각합니다.',\n",
       " '혁신 브레인스토밍 새로운 아이디어는 편안한 영역을 벗어나는 활동이며 이들이 변화의 타당성을 인정하기 위해서는 존경이 동반되어야 합니다.',\n",
       " '하지만 세부사항이 제시되고 계획이 수립되면 현실주의자는 이러한 아이디어를 수행하고 변화하는 것에 있어 핵심적인 역할을 담당합니다.',\n",
       " '현실주의자 관리자 현실주의자들은 책임감으로부터 발생하는 에너지를 좋아합니다.',\n",
       " '이 들은 의무를 다하기 위해 스스로를 압박하면서 더 좋은 결과를 만들어 내는 경우가 많기 때문에 직원들도 같은 수준의 책임감과 헌신을 보여주기를 바랍니다.',\n",
       " '동시에 규칙에 따르는 것을 선호하고 위계질서를 고수하며 혁신을 선호하지 않습니다.',\n",
       " '그래서 정해진 순서를 벗어나는 행동을 할 경우에는 반드시 타당성을 보여주고 결과를 입증해야 합니다.',\n",
       " '일단 움직이고 나서 나중에 허락을 구하는 게 나은 경우도 많지만 현실주의자 관리자에게는 해당된다고 말하기 어렵습니다.',\n",
       " '왜냐하면 이들은 직원들이 자신의 의무를 벗어나는 것을 싫어하며 정해진 계획대로 추진하는 것을 선호하기 때문입니다.',\n",
       " '진실 이 감정적인 민감성보다 더욱 중요하게 생각하는 현실주의자들은 엄격한 비판을 할 수 있으며 복종하지 않는 직원들의 모습을 큰 문제를 일으키는 요인으로 바라보기도 합니다.',\n",
       " 'istj 현실주의자 conclusion 결론 포스팅으로 이동']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9986, 0.0014]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(out,dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
